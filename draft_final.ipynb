{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c8b5357",
   "metadata": {},
   "source": [
    "<u>Business Understanding</u>  \n",
    "  \n",
    "Financial institutions lose billions of dollars each year because of credit card fraud. As the number of credit card transaction increase each year, the rick of fradulant transaction increseas as well. In order to tackle this problem, financial institutes must work on detecting fradulent transaction as early as possible to decrease their losses and maintain their customers safety and trust.\n",
    "\n",
    "The goal of this project is to develop a model that can detect fradulant transaction based on past data. The requirment for the model are:\n",
    "<ul>\n",
    "<li> Identiy the fradulant transaction with high accuracy </li>\n",
    "<li> Decrease the probabily of flase positive of fradulant transaction </li>\n",
    "<li> The model should be fast and scalable that can process large data and detect fradulant transaction close to real-time. </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e54ec",
   "metadata": {},
   "source": [
    "<u>Data Understanding </u>  \n",
    "  \n",
    "  \n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.  \n",
    "  \n",
    "It contains only numerical input variables which are the result of a PCA (Princial Component Analysis) transformation. By using PCA, the correlated features in the original data are comdined and new features are created labeled: \n",
    "<li>V1</li>\n",
    "<li>V2</li>\n",
    "<li>till V28</li>  \n",
    "\n",
    "which simplyfies the data set and adds privacy to the data. In addition to the PCA transformet feature, it includes Time, Amount and Class feater. Class is our target variable, 0 indicating legimate and 1 indicating fradulant transaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c85de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature of the data set\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../creditcard.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652c2547",
   "metadata": {},
   "source": [
    "<u>Data Preparation</u>  \n",
    "  \n",
    "To prepare the data set for effective modeling, we will first check if the data has any missing values. If the data contains missing values and the feature are importatnt and strongly influece the target variable, we will impute the missing value with the median value of the respective feature.  \n",
    "Next we will check for class imbalance. If the feature or traget variable has class imbalance it can lead to misleading accuracy. Lastly, since the PCA transformet features are already numerical and standardized, we need to standarize the remaning features to ensure that all the features contribute equally to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d86a6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing vlaues\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668f6d13",
   "metadata": {},
   "source": [
    "Since we do not have  missing value in the features, we can proceed to next step of data preparation, handeling class imbalance. Since most of our feature are numerical values, we only need to check for class imbalance in our traget varaible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e71c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    99.827251\n",
      "1     0.172749\n",
      "Name: Class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# checking for class imbalance\n",
    "print(df['Class'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8fc30c",
   "metadata": {},
   "source": [
    "The traget variable is havily imbalanced. To address class imbalance we can use resampling method, which includes oversampling the minorty class using SMOTE or alternatively, use models that can handel class imbalance during training.  \n",
    "The class imbalance will be handeled according the model that will be used later.  \n",
    "  \n",
    "Next step is the standardize the features. It transfrome the features to have mean 0 and standard deviation to 1, which ensure the features are are in a same scale and they help models perform better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2510a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                mean       std         min         max\n",
      "V1      1.168375e-15  1.958696  -56.407510    2.454930\n",
      "V2      3.416908e-16  1.651309  -72.715728   22.057729\n",
      "V3     -1.379537e-15  1.516255  -48.325589    9.382558\n",
      "V4      2.074095e-15  1.415869   -5.683171   16.875344\n",
      "V5      9.604066e-16  1.380247 -113.743307   34.801666\n",
      "V6      1.487313e-15  1.332271  -26.160506   73.301626\n",
      "V7     -5.556467e-16  1.237094  -43.557242  120.589494\n",
      "V8      1.213481e-16  1.194353  -73.216718   20.007208\n",
      "V9     -2.406331e-15  1.098632  -13.434066   15.594995\n",
      "V10     2.239053e-15  1.088850  -24.588262   23.745136\n",
      "V11     1.673327e-15  1.020713   -4.797473   12.018913\n",
      "V12    -1.247012e-15  0.999201  -18.683715    7.848392\n",
      "V13     8.190001e-16  0.995274   -5.791881    7.126883\n",
      "V14     1.207294e-15  0.958596  -19.214325   10.526766\n",
      "V15     4.887456e-15  0.915316   -4.498945    8.877742\n",
      "V16     1.437716e-15  0.876253  -14.129855   17.315112\n",
      "V17    -3.772171e-16  0.849337  -25.162799    9.253526\n",
      "V18     9.564149e-16  0.838176   -9.498746    5.041069\n",
      "V19     1.039917e-15  0.814041   -7.213527    5.591971\n",
      "V20     6.406204e-16  0.770925  -54.497720   39.420904\n",
      "V21     1.654067e-16  0.734524  -34.830382   27.202839\n",
      "V22    -3.568593e-16  0.725702  -10.933144   10.503090\n",
      "V23     2.578648e-16  0.624460  -44.807735   22.528412\n",
      "V24     4.473266e-15  0.605647   -2.836627    4.584549\n",
      "V25     5.340915e-16  0.521278  -10.295397    7.519589\n",
      "V26     1.683437e-15  0.482227   -2.604551    3.517346\n",
      "V27    -3.660091e-16  0.403632  -22.565679   31.612198\n",
      "V28    -1.227390e-16  0.330083  -15.430084   33.847808\n",
      "Amount  2.913952e-17  1.000002   -0.353229  102.362243\n",
      "Class   1.727486e-03  0.041527    0.000000    1.000000\n"
     ]
    }
   ],
   "source": [
    "print(df.describe().T[['mean', 'std', 'min', 'max']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50abc9d5",
   "metadata": {},
   "source": [
    "The PCA features are already standarized, only Time, Amount are not standarize. Since time is the number of seconds elapsed between this transaction and the first transaction in the dataset, we will be droping the feature. So, the only remaining feature to standarize is the Aoumnt feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf13e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping the time cloumn\n",
    "df = df.drop(columns=['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9990e13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                mean       std         min         max\n",
      "V1      1.168375e-15  1.958696  -56.407510    2.454930\n",
      "V2      3.416908e-16  1.651309  -72.715728   22.057729\n",
      "V3     -1.379537e-15  1.516255  -48.325589    9.382558\n",
      "V4      2.074095e-15  1.415869   -5.683171   16.875344\n",
      "V5      9.604066e-16  1.380247 -113.743307   34.801666\n",
      "V6      1.487313e-15  1.332271  -26.160506   73.301626\n",
      "V7     -5.556467e-16  1.237094  -43.557242  120.589494\n",
      "V8      1.213481e-16  1.194353  -73.216718   20.007208\n",
      "V9     -2.406331e-15  1.098632  -13.434066   15.594995\n",
      "V10     2.239053e-15  1.088850  -24.588262   23.745136\n",
      "V11     1.673327e-15  1.020713   -4.797473   12.018913\n",
      "V12    -1.247012e-15  0.999201  -18.683715    7.848392\n",
      "V13     8.190001e-16  0.995274   -5.791881    7.126883\n",
      "V14     1.207294e-15  0.958596  -19.214325   10.526766\n",
      "V15     4.887456e-15  0.915316   -4.498945    8.877742\n",
      "V16     1.437716e-15  0.876253  -14.129855   17.315112\n",
      "V17    -3.772171e-16  0.849337  -25.162799    9.253526\n",
      "V18     9.564149e-16  0.838176   -9.498746    5.041069\n",
      "V19     1.039917e-15  0.814041   -7.213527    5.591971\n",
      "V20     6.406204e-16  0.770925  -54.497720   39.420904\n",
      "V21     1.654067e-16  0.734524  -34.830382   27.202839\n",
      "V22    -3.568593e-16  0.725702  -10.933144   10.503090\n",
      "V23     2.578648e-16  0.624460  -44.807735   22.528412\n",
      "V24     4.473266e-15  0.605647   -2.836627    4.584549\n",
      "V25     5.340915e-16  0.521278  -10.295397    7.519589\n",
      "V26     1.683437e-15  0.482227   -2.604551    3.517346\n",
      "V27    -3.660091e-16  0.403632  -22.565679   31.612198\n",
      "V28    -1.227390e-16  0.330083  -15.430084   33.847808\n",
      "Amount -1.596686e-17  1.000002   -0.353229  102.362243\n",
      "Class   1.727486e-03  0.041527    0.000000    1.000000\n"
     ]
    }
   ],
   "source": [
    "# Standarizing the Aoumnt folumn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df['Amount'] = scaler.fit_transform(df[['Amount']])\n",
    "\n",
    "print(df.describe().T[['mean', 'std', 'min', 'max']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1307ec",
   "metadata": {},
   "source": [
    "<u>Modeling</u>  \n",
    "  \n",
    "We will be experiment with various model for this data set to identify the best approach to tackle this problem. After evaluating all the models performance using appropriate metric, the model that performs the best will be our final model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae658ba4",
   "metadata": {},
   "source": [
    "We will use Logistic regression as our baseline model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfaa642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     85295\n",
      "           1       0.06      0.88      0.12       148\n",
      "\n",
      "    accuracy                           0.98     85443\n",
      "   macro avg       0.53      0.93      0.55     85443\n",
      "weighted avg       1.00      0.98      0.99     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=['Class'])\n",
    "y = df['Class']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Initialize and train Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=42, C=0.01, solver='lbfgs', max_iter=1000)\n",
    "log_reg.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate model performance\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cf00e8",
   "metadata": {},
   "source": [
    "The logistic regression model was able to detect non fradulant transaction  really well, but the model struggled while detecting fradualant transaction with the precision of only 6%, which mean that 94% were false positive. However, the recall for the fradualant transaction is 88%, which means 88% of fraud was caught.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cdda3b",
   "metadata": {},
   "source": [
    "Our next model is XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45aed0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.90      0.87      0.89        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.95      0.93      0.94     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Split data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=1)\n",
    "\n",
    "# Calculate class imbalance ratio to help the model handle imbalanced data.\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "model = XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,  \n",
    "    objective='binary:logistic',\n",
    "    eval_metric='aucpr',              \n",
    "    n_estimators=400,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ae1afb",
   "metadata": {},
   "source": [
    "The XGBoost model precission and recall for non fradulant transaction are 100%. For the fradulant transaction, the model showed good reasuls of 90% precisioon and 87% recall. Overall the model achieved good reasult on classifing in this data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e768821",
   "metadata": {},
   "source": [
    "Our next model ....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e57d988",
   "metadata": {},
   "source": [
    "<u>Evulation</u>  \n",
    "\n",
    "Need to write the evulation after all the model are added"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69461298",
   "metadata": {},
   "source": [
    "<u>Deployment</u>  \n",
    "  \n",
    "After builing the Machine learning model, we can deploy the model using the following steps.\n",
    "<li><b>Train and save the model</b>: After training the model, we need to save the model as a file, eg: .pkl, .joblib. These files help save the model with all the learned parametere, so we can load and use it exactly as it was</li>\n",
    "<li><b>Stroe the model on the server</b>: Store the file in the server from where we will run our API. The API will load the file from the server when it receives a request.\n",
    "<li><b>Create an API</b>: Develop an API the take in input, loads the model, runs the prediction and retuns the result.  \n",
    "    \n",
    "      \n",
    "\n",
    "<b>Another Method is to Dockerze the model</b>\n",
    "<li><b>Contanerize the API</b>: Using Docker we need to contianer the API, model and dependencies into a single docker. This ensures or model runs consitently accross diffent enviroment.\n",
    "<li><b>Share the Docker image</b>: Uload the image to Docker hub for easier distribution or deploy it to a cloud, eg: AWS, Azure.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aai510",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
